{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461b0d35-45e1-4ddb-8e4c-abb1eb66c021",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83822221-4804-4bc2-af18-7e1269037caa",
   "metadata": {},
   "source": [
    "\n",
    "An **RNN** is a type of neural network made for handling **sequences** — like sentences, time-series data, or audio.  \n",
    "Unlike normal networks that treat every input independently, RNNs **remember past information** using something called a **hidden state**.\n",
    "\n",
    "---\n",
    "\n",
    "###  How It Works\n",
    "At each time step:\n",
    "\n",
    "hₜ = activation(Wₓₕ · xₜ + Wₕₕ · hₜ₋₁ + bₕ)\n",
    "yₜ = Wₕᵧ · hₜ + bᵧ\n",
    "\n",
    "Where:\n",
    "- Input → `xₜ`  \n",
    "- Previous memory → `hₜ₋₁`  \n",
    "- Updated memory → `hₜ`  \n",
    "- Output → `yₜ`\n",
    "`Wₓₕ`, `Wₕₕ`, `Wₕᵧ` → weight matrices  \n",
    "`bₕ`, `bᵧ` → bias terms  \n",
    "`activation()` → non-linear function such as *tanh* or *ReLU*\n",
    "\n",
    "\n",
    "Basically, it reads one input at a time, updates its memory, and produces an output — kind of like how we process words in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "###  Quick PyTorch Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "x = torch.randn(5, 3, 10)  # (batch, seq_len, input_size)\n",
    "\n",
    "output, hidden = rnn(x)\n",
    "print(output.shape, hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5fe4ed-3a66-4491-b21a-fa68dd66fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root=Path(\"data\")\n",
    "root.mkdir(exist_ok=True)\n",
    "path=Path(root)/\"100_Unique_QA_Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2861fba-9c36-463f-a136-079c71eebb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b835b-4044-4224-90ed-7ac1495c8b49",
   "metadata": {},
   "source": [
    "### Tokenization Function\n",
    "\n",
    "The following function takes a text input, converts it to lowercase, removes certain punctuation marks (`?` and `'`), and then splits the text into individual tokens (words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fd4c1b-e53d-4ce3-8a1a-645f30f754b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "  text = text.lower()\n",
    "  text = text.replace('?','')\n",
    "  text = text.replace(\"'\",\"\")\n",
    "  return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964f0a23-e86b-4abb-bdcd-9c257fc237b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb663e-d29c-4108-b0eb-a10066297dea",
   "metadata": {},
   "source": [
    "### lets create dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1706767c-dcf7-4fa5-9ac7-6039e2acd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab\n",
    "vocab={'<UNK>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c163b86-9098-4e78-8984-d037fbf9df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "  tokenized_question = tokenize(row['question'])\n",
    "  tokenized_answer = tokenize(row['answer'])\n",
    "\n",
    "  merged_tokens = tokenized_question + tokenized_answer\n",
    "\n",
    "  for token in merged_tokens:\n",
    "\n",
    "    if token not in vocab:\n",
    "      vocab[token] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00fd0f2-e314-4505-b2ed-6bc0846e08d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b2089e-1e7c-425b-956a-95b1ddd189a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53dd75d-fe34-41ec-be67-ef92d331a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3391f2-46ac-43a7-b220-61ae14132aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed9dc71-eae6-4ce5-9aca-c95aed6a2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to numerical indices\n",
    "def text_to_indices(text, vocab):\n",
    "\n",
    "  indexed_text = []\n",
    "\n",
    "  for token in tokenize(text):\n",
    "\n",
    "    if token in vocab:\n",
    "      indexed_text.append(vocab[token])\n",
    "    else:\n",
    "      indexed_text.append(vocab['<UNK>'])\n",
    "\n",
    "  return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b47a5f-9197-46b6-8c54-8d0dffb40c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"What what is campusx\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2d0d9b-c50a-4434-bb31-86691705c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a072fd5-5f75-4705-8840-e0ad4b37fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "\n",
    "  def __init__(self, df, vocab):\n",
    "    self.df = df\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    numerical_question = text_to_indices(self.df.iloc[index]['question'], self.vocab)\n",
    "    numerical_answer = text_to_indices(self.df.iloc[index]['answer'], self.vocab)\n",
    "\n",
    "    return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c114fb-1992-4fa2-beca-dc17e09c0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65799e91-15f8-4f4f-a7fd-c952c79249c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 2, 3, 4, 5, 6]), tensor([7]))\n",
      "(tensor([1, 2, 3, 4, 5, 8]), tensor([9]))\n",
      "(tensor([10, 11, 12, 13, 14, 15]), tensor([16]))\n",
      "(tensor([ 1,  2,  3, 17, 18, 19, 20, 21, 22]), tensor([23]))\n",
      "(tensor([ 1,  2,  3, 24, 25,  5, 26, 19, 27]), tensor([28]))\n",
      "(tensor([10, 29,  3, 30, 31]), tensor([32]))\n",
      "(tensor([ 1,  2,  3, 33, 34,  5, 35]), tensor([36]))\n",
      "(tensor([ 1,  2,  3, 37, 38, 39, 40]), tensor([41]))\n",
      "(tensor([42, 43, 44, 45, 46, 47, 48]), tensor([49]))\n",
      "(tensor([ 1,  2,  3, 50, 51, 19,  3, 45]), tensor([52]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295d15c1-1d3d-4409-91eb-9957ce0d3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826e7116-f99a-41b9-8280-1a8edd7ca45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e6bb832-a6cf-4f5a-802a-7b3ca73b97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n"
     ]
    }
   ],
   "source": [
    "for question, answer in dataloader:\n",
    "  print(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "181162e8-e4fc-445f-ac0e-adb2516bbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e5927-c33f-4bcb-87ab-fcb59c18206f",
   "metadata": {},
   "source": [
    "##  Understanding nn.Embedding\n",
    "\n",
    "**nn.Embedding** is like a **lookup table** for discrete items (words, categories, etc.).  \n",
    "Instead of huge one-hot vectors, each item gets a **dense vector** of numbers which the model can learn during training.\n",
    "\n",
    "\n",
    "\n",
    "### Quick Theory\n",
    "- **num_embeddings** -> number of unique items (vocab size).  \n",
    "- **embedding_dim** -> length of vector for each item (how much info you store per item).  \n",
    "- Embedding vectors are **trainable** and updated via backprop.  \n",
    "- Bigger `embedding_dim` -> more capacity to capture patterns, but does **not** directly set final output size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290d56f4-88c2-4543-a9e7-eccfcaa53813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape : Embedding(300, 2)\n",
      "length of the vector for each item:  20\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]])\n",
      "Max index: tensor(115)\n",
      "Min index: tensor(1)\n",
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6092, -0.9178],\n",
       "         [-0.4126,  0.0110],\n",
       "         [-0.7848,  0.5087],\n",
       "         [ 0.6012,  0.1675],\n",
       "         [-0.3275, -0.1840],\n",
       "         [ 0.1951,  0.0170],\n",
       "         [-0.5300, -0.3180]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=nn.Embedding(300, embedding_dim=2)\n",
    "print(\"output shape :\",emb)\n",
    "print(\"length of the vector for each item: \",20)\n",
    "print(question)\n",
    "print(\"Max index:\", question.max())\n",
    "print(\"Min index:\", question.min())\n",
    "print(question.shape)\n",
    "embed=emb(question)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2cbf9f5-7545-41ae-9b0e-2ada02bf5f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6]) torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 50]),\n",
       " tensor([[ 0.6268,  0.8933, -1.2490, -1.8061, -0.5711,  1.2475, -2.0431,  1.6316,\n",
       "           1.9809,  0.2070, -0.8729, -0.3445, -1.0653,  1.0728, -1.7473,  0.5859,\n",
       "           0.5347,  0.2896,  0.9558, -0.6758, -0.8886, -1.9650,  0.7773,  1.2955,\n",
       "          -0.1048, -0.8204, -1.2446,  0.2566, -0.2134,  0.4578,  1.9231, -1.2731,\n",
       "           1.1389, -0.5887, -0.1669, -1.4945, -2.6910,  0.1779,  0.1468, -0.4423,\n",
       "           0.4099,  0.5296,  1.0011,  0.3612,  1.4854, -1.5883, -0.5590, -0.4330,\n",
       "          -0.1028, -0.5035],\n",
       "         [-0.5056,  0.0574, -1.0703, -1.0891, -0.5065,  0.4366, -0.5490, -0.7197,\n",
       "          -1.1399,  0.0832,  1.0241, -0.5500, -0.0943,  0.3651,  0.5156,  1.2315,\n",
       "           0.6025, -0.7092, -1.0356, -1.4081, -0.5784,  0.2442, -0.7939,  1.0548,\n",
       "          -0.1682, -0.2497,  1.3237,  0.9805,  0.1756, -1.4635, -1.3837, -1.1347,\n",
       "          -0.3536, -1.3348,  0.3395, -1.2854,  0.4777,  1.3339, -1.1026, -1.8654,\n",
       "           0.2018,  0.3537,  0.4909,  1.0389, -0.6361,  0.1491,  0.2743,  0.5613,\n",
       "          -0.8833,  0.9307],\n",
       "         [ 0.6568,  0.2341,  0.0276,  1.9479, -1.1557, -1.6972, -0.0626,  0.7210,\n",
       "           0.4465,  0.6278, -0.8268,  0.5889, -0.2450, -0.1834, -0.1082, -0.7991,\n",
       "           0.8600,  0.0070,  1.0909,  0.5284,  0.9671, -0.7582,  0.8531,  0.3890,\n",
       "           0.0210, -1.1612,  0.3343, -0.4496,  1.6121,  0.8836, -1.0546, -0.2775,\n",
       "          -0.9446, -0.5411, -1.1571,  0.7490, -0.5613, -0.0308,  0.2240, -0.0962,\n",
       "          -0.3980,  2.9156, -0.7689,  0.5618,  0.3483, -0.9817,  0.4492,  0.9339,\n",
       "           1.3003, -0.8295],\n",
       "         [-0.5656,  0.4877,  0.9917,  0.4896,  1.1527,  0.9684,  1.2261, -0.0749,\n",
       "          -1.3563, -0.4526,  1.0069,  1.1290,  0.8580,  0.7035,  0.5946, -0.3524,\n",
       "           0.1306,  1.2717,  2.1918,  1.0431,  0.9530,  0.2984,  0.2824,  0.8395,\n",
       "           0.6881,  1.0028, -0.8320,  0.1803,  0.1425,  0.9485,  1.8726,  0.1139,\n",
       "           0.1711, -0.4944,  1.1446, -0.8404, -1.8545,  1.3379,  0.7031,  0.9412,\n",
       "           0.7893,  0.7072, -0.0410, -0.1666, -0.1387, -0.9612,  0.6781,  0.7384,\n",
       "           1.3179, -0.9759],\n",
       "         [-0.5727, -1.0908,  0.2341, -1.6619, -0.3576,  0.3832, -1.1588, -0.0243,\n",
       "           0.3797,  1.9713, -0.2513, -0.6456, -1.9194, -0.2963, -0.9735, -1.0738,\n",
       "          -0.3239,  1.3657, -0.9918,  0.7082, -0.5204,  1.1016, -0.0942,  0.7461,\n",
       "           0.6158,  1.4557, -0.0263, -0.2114,  0.3181, -1.0095, -0.4677,  0.3027,\n",
       "           1.1249, -0.5459, -0.8030, -0.7576, -0.0906, -1.8843,  1.7151,  1.6253,\n",
       "           2.5939,  0.7155,  1.8016, -0.3839, -1.2906,  2.2801, -0.1958,  0.3980,\n",
       "           0.2689,  1.1861],\n",
       "         [ 1.5369, -0.0571,  0.3926, -0.3494,  1.2400,  0.9659,  0.0398, -0.2391,\n",
       "           1.2188, -1.6712, -0.8221,  1.5733, -0.8197, -0.0930,  0.0201, -0.2934,\n",
       "           1.9459,  0.4002, -0.5331,  1.6223,  0.7863, -0.4767, -1.0338,  0.0879,\n",
       "           0.1315, -0.1691,  0.4865,  0.8437,  0.7889, -0.5016, -0.1194,  0.6592,\n",
       "           0.7116, -0.9190, -0.2918,  0.7253,  1.1143,  0.1791, -0.2172, -0.6370,\n",
       "           0.2839, -0.1403, -1.6417, -0.1699, -0.5596, -0.0276, -0.3925,  1.4710,\n",
       "           0.9083, -1.2378]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=nn.Embedding(324, embedding_dim=50)\n",
    "print(dataset[0][0],dataset[0][0].shape)\n",
    "a=x(dataset[0][0])\n",
    "a.shape,a\n",
    "#x has total 6 elements so 6,50 is shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "869693d3-21ea-4827-a56b-05a9e3f086bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.2326e-01,  5.2824e-01, -4.3203e-01,  3.0618e-01,  7.3766e-01,\n",
       "          -2.4961e-01, -5.7677e-01,  6.0965e-02,  2.6584e-01,  4.7650e-01,\n",
       "          -3.2580e-01, -7.2345e-01,  3.3578e-01, -7.8081e-01, -7.0934e-01,\n",
       "           3.0666e-01,  2.1000e-01,  8.0719e-01, -2.8237e-01,  8.5400e-01,\n",
       "           4.6043e-01,  1.9781e-01,  3.5791e-01,  3.0283e-01,  5.6862e-01,\n",
       "          -5.2513e-01,  6.5580e-01,  5.1689e-01,  7.2570e-01, -5.9893e-01,\n",
       "           7.4059e-01, -5.6910e-01,  4.6036e-01,  1.7694e-01,  6.0803e-01,\n",
       "           8.4658e-01, -3.0039e-01, -2.0616e-01, -2.4237e-01,  6.8960e-01,\n",
       "           1.1245e-01,  5.0320e-01, -3.0582e-02,  4.0742e-01,  6.0274e-01,\n",
       "           5.0528e-01, -7.1713e-01,  6.7248e-01, -6.4982e-01,  4.8335e-01,\n",
       "          -4.4291e-01,  3.9940e-01, -6.8556e-01, -3.5211e-01, -6.6234e-01,\n",
       "          -2.6362e-01,  2.5123e-01, -8.7922e-01,  4.1288e-01, -5.5437e-02,\n",
       "          -7.8069e-01, -8.1894e-03,  1.8690e-01, -1.1994e-01],\n",
       "         [-6.4569e-02,  3.7943e-02,  7.9348e-01, -2.8321e-01,  4.8863e-01,\n",
       "          -5.5221e-01,  2.5744e-02,  5.6526e-01,  2.6112e-01,  7.2155e-01,\n",
       "           4.6514e-01,  3.2306e-01, -5.0225e-01, -3.3146e-01, -3.3441e-01,\n",
       "           2.3815e-01,  6.8788e-01,  6.7031e-01,  2.0884e-01, -1.5821e-03,\n",
       "          -4.0458e-01,  3.3972e-01, -2.4723e-01, -6.5115e-01,  2.5017e-01,\n",
       "          -1.7526e-01,  5.6525e-01,  5.1468e-01, -3.4096e-01,  3.9275e-01,\n",
       "           7.3236e-01,  4.2139e-01, -4.1161e-01, -8.1735e-02, -6.5983e-01,\n",
       "           6.8144e-01,  8.2289e-01,  2.2017e-01, -2.4018e-02, -9.4427e-02,\n",
       "           5.9752e-01,  1.9064e-02, -8.0041e-01, -4.7994e-01, -1.2877e-01,\n",
       "           6.1007e-01, -1.1327e-01, -4.7380e-01, -8.9117e-01, -2.1505e-01,\n",
       "          -8.3160e-01,  7.7036e-01, -1.9702e-01,  8.9039e-01, -5.0648e-01,\n",
       "          -8.3480e-03,  3.6240e-01, -7.2044e-01, -1.0732e-01,  1.3395e-02,\n",
       "           1.1140e-01, -1.1163e-01,  3.4720e-01,  7.4448e-01],\n",
       "         [-2.3932e-01,  2.9515e-01,  1.6746e-01,  7.7050e-01,  1.2016e-01,\n",
       "           7.9070e-01, -2.5589e-01, -3.9224e-01,  1.7568e-01, -1.6238e-01,\n",
       "           8.8061e-02,  6.0883e-02,  3.1562e-01, -5.6736e-01, -6.3185e-02,\n",
       "          -5.4176e-01,  3.4639e-02,  2.5433e-01,  1.1054e-01,  7.3823e-01,\n",
       "          -2.1095e-01, -1.1424e-01,  1.8014e-01, -4.7333e-01,  6.4157e-01,\n",
       "          -6.3031e-01, -1.9674e-01, -3.0657e-01, -1.4749e-01,  6.8488e-01,\n",
       "          -4.3839e-01,  9.2132e-01,  8.7240e-01,  3.8301e-01,  3.5140e-01,\n",
       "           5.1312e-01, -7.2819e-01, -5.8560e-01,  2.4043e-01,  2.4419e-01,\n",
       "           2.0934e-02,  1.5280e-01, -1.4287e-02,  4.6838e-01, -7.0199e-01,\n",
       "           7.2282e-01, -7.7806e-02, -2.5633e-01, -4.6965e-01,  1.5545e-01,\n",
       "           1.2397e-02, -2.0686e-01,  5.8539e-01, -6.0258e-02,  3.8957e-02,\n",
       "           1.8973e-01, -1.4889e-01, -7.8679e-01,  7.4188e-01,  3.1530e-01,\n",
       "           5.6202e-01, -7.3203e-01, -3.7197e-01, -3.7008e-01],\n",
       "         [-3.7959e-01,  4.1156e-01, -3.0470e-01,  5.0546e-02,  1.3956e-01,\n",
       "           3.4858e-02,  4.4686e-01, -6.8726e-01,  2.5054e-01, -1.5481e-01,\n",
       "           1.3514e-01, -6.2066e-02, -2.5812e-01, -3.7315e-01, -5.6723e-01,\n",
       "          -6.1221e-01, -2.2517e-01,  6.3441e-01, -5.7257e-01,  5.8232e-01,\n",
       "          -6.0828e-01,  7.8663e-01,  1.0421e-01,  3.3171e-01,  1.0949e-01,\n",
       "          -7.2390e-02,  3.7691e-01,  7.2023e-02,  7.7747e-01,  6.3931e-01,\n",
       "          -6.1558e-01, -1.7207e-01, -2.7091e-01,  2.2508e-01, -1.9856e-01,\n",
       "          -7.0420e-04,  1.1525e-02,  2.4247e-01, -1.7966e-01,  1.5625e-01,\n",
       "           2.1989e-01, -1.3329e-01, -4.5057e-01, -2.3419e-01, -4.6091e-01,\n",
       "           4.5859e-01, -4.0362e-02,  5.9561e-01,  4.7248e-01, -2.4598e-01,\n",
       "          -3.9823e-01,  4.2349e-01, -2.1244e-01,  4.7122e-01,  6.9477e-01,\n",
       "          -2.8465e-01, -1.2311e-01, -4.1916e-01,  2.7414e-01, -2.9752e-01,\n",
       "          -3.7895e-01, -3.0665e-01, -5.1130e-01, -8.9313e-01],\n",
       "         [ 7.1599e-01, -6.2580e-02,  4.1937e-01, -1.5662e-01,  2.8370e-01,\n",
       "          -2.9264e-01,  7.9155e-01,  8.2557e-01, -5.2211e-01,  5.9869e-01,\n",
       "          -8.6226e-01,  9.8742e-02,  7.8415e-01, -3.3605e-01, -7.1222e-01,\n",
       "          -1.1613e-01,  6.4013e-01,  5.4549e-02, -1.9054e-01, -5.1788e-01,\n",
       "           1.5330e-01,  1.8143e-01, -2.4494e-01,  2.9664e-02, -2.7836e-02,\n",
       "           8.7600e-01,  5.3744e-01,  8.2552e-01, -4.8880e-01,  3.3926e-01,\n",
       "           4.9456e-01, -7.9245e-01, -4.7567e-01, -6.2032e-02, -7.4117e-01,\n",
       "           4.1697e-01,  8.5292e-01,  1.9482e-01, -3.5335e-01, -5.4318e-01,\n",
       "          -6.2163e-01,  5.8692e-01,  9.0164e-02,  1.6158e-01,  4.3570e-01,\n",
       "          -1.7457e-01,  2.7019e-01,  5.6188e-01, -2.0464e-01, -8.3972e-04,\n",
       "          -7.0423e-01, -6.2770e-01,  6.3141e-01, -1.7675e-01,  1.2231e-01,\n",
       "           7.8503e-01, -5.5601e-01, -2.1959e-01,  3.9627e-01, -1.5022e-01,\n",
       "          -2.3728e-01,  5.9569e-01,  2.5625e-01, -2.4019e-01],\n",
       "         [ 1.8902e-02, -2.1725e-01, -6.6240e-01, -3.9847e-01,  3.3090e-01,\n",
       "           3.9829e-01,  6.7645e-01,  3.4215e-01,  1.1226e-01, -6.3466e-02,\n",
       "          -2.1045e-01,  5.9406e-02, -2.2700e-01,  1.7639e-01, -3.8083e-01,\n",
       "          -1.5105e-01,  3.0667e-01,  3.2190e-01, -6.9972e-02,  3.5082e-01,\n",
       "          -2.5130e-01,  1.8723e-01, -8.7254e-01,  2.7604e-01,  2.9141e-01,\n",
       "           4.0534e-01, -5.3428e-01,  2.8587e-01, -1.8770e-01,  6.4914e-01,\n",
       "          -4.4241e-01,  7.8857e-01, -2.5210e-01,  7.7509e-01,  5.9875e-01,\n",
       "           1.4368e-01,  4.7978e-01,  4.0810e-01,  1.6525e-01,  5.4937e-02,\n",
       "          -5.3346e-01,  5.8142e-01,  3.7324e-01, -2.2409e-01,  5.0096e-01,\n",
       "           7.1737e-01,  2.4325e-01, -7.0557e-01,  7.7242e-01, -1.4942e-01,\n",
       "          -2.6444e-01,  1.8841e-01, -2.3602e-01,  2.7332e-01,  3.8904e-01,\n",
       "          -3.1354e-02,  7.0607e-01, -1.0802e-01,  9.2378e-01, -1.6616e-01,\n",
       "           6.7001e-01, -3.2335e-01, -7.4582e-01,  1.2375e-01]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.0189, -0.2172, -0.6624, -0.3985,  0.3309,  0.3983,  0.6765,  0.3421,\n",
       "           0.1123, -0.0635, -0.2104,  0.0594, -0.2270,  0.1764, -0.3808, -0.1511,\n",
       "           0.3067,  0.3219, -0.0700,  0.3508, -0.2513,  0.1872, -0.8725,  0.2760,\n",
       "           0.2914,  0.4053, -0.5343,  0.2859, -0.1877,  0.6491, -0.4424,  0.7886,\n",
       "          -0.2521,  0.7751,  0.5987,  0.1437,  0.4798,  0.4081,  0.1653,  0.0549,\n",
       "          -0.5335,  0.5814,  0.3732, -0.2241,  0.5010,  0.7174,  0.2433, -0.7056,\n",
       "           0.7724, -0.1494, -0.2644,  0.1884, -0.2360,  0.2733,  0.3890, -0.0314,\n",
       "           0.7061, -0.1080,  0.9238, -0.1662,  0.6700, -0.3234, -0.7458,  0.1237]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=nn.RNN(50,64)\n",
    "b=y(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b577b0f6-5555-4e21-9d18-9b879f997e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape,b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6172d927-4cd9-4419-8127-d69d42650cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b[0] is hidden state\n",
    "#b[1] is output state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f5d04ee-022f-47e5-bec8-1d9b2cbb1f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7027e-01, -3.2306e-02, -2.0158e-01, -5.2658e-02, -2.7331e-01,\n",
       "         -1.3031e-02, -9.8036e-02,  3.0786e-01,  6.2587e-01, -6.0868e-02,\n",
       "          1.3998e-01,  2.7152e-01, -2.9447e-01,  2.8547e-01, -5.2427e-02,\n",
       "          3.1036e-01, -1.9277e-01,  2.9946e-01, -1.9051e-01, -8.8082e-02,\n",
       "         -2.6671e-01,  3.5434e-01, -9.5855e-03, -3.7849e-01,  2.3101e-01,\n",
       "         -6.1108e-02,  4.0075e-01,  3.1820e-01, -1.0222e-01,  1.6391e-01,\n",
       "         -8.3096e-02, -6.2626e-02,  2.8781e-01, -2.9206e-01,  2.6283e-01,\n",
       "         -9.0673e-02,  1.2023e-01, -5.9162e-01,  2.2173e-03, -1.2746e-01,\n",
       "         -6.7945e-02,  4.6058e-02, -2.7778e-01, -5.5458e-03,  3.9478e-01,\n",
       "          2.4499e-01,  2.7286e-01, -4.0879e-02,  1.9903e-01,  2.4198e-01,\n",
       "          1.2738e-01, -1.9824e-01,  2.5242e-01, -5.1522e-01,  2.7397e-01,\n",
       "         -1.3139e-01,  2.4400e-02,  6.1550e-02,  1.8925e-01, -3.5604e-02,\n",
       "          2.5617e-02, -8.1841e-02,  6.0330e-02, -1.9966e-01, -6.2148e-02,\n",
       "         -3.4615e-01,  3.8530e-02,  1.2644e-01,  1.8048e-01, -3.6826e-02,\n",
       "         -3.7859e-01, -7.2024e-01,  1.6528e-01, -9.4193e-03, -7.2081e-02,\n",
       "         -7.2009e-02, -2.0230e-01, -1.5842e-01,  1.0715e-01, -6.2090e-01,\n",
       "         -8.8193e-02, -5.5182e-02, -3.1434e-01,  3.2415e-01, -8.8329e-02,\n",
       "         -1.4245e-02, -3.4313e-01,  6.0281e-02,  1.1844e-01, -1.9571e-01,\n",
       "          1.7513e-01,  4.2948e-01,  1.0111e-01, -4.3531e-01, -7.5965e-02,\n",
       "          6.9049e-02,  2.8475e-01,  2.5619e-01, -2.8272e-01,  1.5100e-01,\n",
       "         -2.6669e-01,  4.0442e-01, -7.8660e-02, -1.2024e-02, -1.3070e-01,\n",
       "          5.3428e-02,  1.2353e-01,  1.5105e-01, -3.0932e-01,  5.2764e-01,\n",
       "         -1.1076e-01,  3.8724e-01, -1.0891e-01,  8.6068e-02,  1.5328e-01,\n",
       "          1.5279e-01,  5.7496e-02,  6.0000e-02,  4.2224e-01,  1.0466e-01,\n",
       "         -1.8524e-01, -4.0048e-01,  7.7963e-02, -2.9861e-01,  2.9074e-02,\n",
       "         -4.4739e-01, -3.3966e-01,  1.8274e-01,  8.1583e-02, -2.3312e-01,\n",
       "         -1.3604e-01,  2.3324e-01,  5.5623e-02, -1.2598e-03,  1.3380e-01,\n",
       "          9.6757e-01,  8.4418e-03, -2.1229e-01, -4.5888e-02, -2.9724e-01,\n",
       "          1.2660e-01,  3.0332e-02, -8.8652e-03,  1.0447e-02, -1.3290e-01,\n",
       "         -2.3471e-01, -2.0392e-01, -3.5471e-01, -2.2149e-01, -3.0015e-01,\n",
       "         -5.8480e-02,  1.7731e-02,  2.0497e-01,  1.7150e-02, -3.4826e-01,\n",
       "         -4.5133e-01,  5.8870e-02, -3.8047e-01, -9.5979e-02, -4.4413e-01,\n",
       "         -3.0800e-03,  2.4358e-01, -3.5576e-01, -9.0660e-02, -4.2746e-01,\n",
       "         -3.0567e-01, -2.2528e-01, -2.3380e-01, -2.1318e-01, -2.8703e-01,\n",
       "         -2.6548e-01, -4.0853e-01,  9.6150e-02, -5.2885e-01, -3.6284e-01,\n",
       "         -4.9485e-01,  1.8181e-01, -2.0036e-01,  1.0373e-01, -2.1847e-01,\n",
       "         -8.2150e-02, -1.4706e-01,  5.7974e-02,  3.9556e-01, -2.1929e-01,\n",
       "          1.6383e-01,  4.2043e-01,  9.5703e-02, -8.8756e-03, -3.9399e-03,\n",
       "          2.5741e-01,  1.6106e-01,  3.9781e-02,  2.7552e-01,  1.5781e-03,\n",
       "         -3.4686e-02, -1.6518e-01, -3.7653e-02,  1.7141e-01,  1.2942e-01,\n",
       "         -3.3774e-01, -1.2076e-01, -1.2145e-01,  4.3696e-01, -1.7580e-01,\n",
       "         -4.2094e-01,  2.3479e-01,  3.4231e-01,  4.1023e-02,  1.6802e-01,\n",
       "         -4.3800e-02,  3.5640e-01, -2.1161e-01,  7.6242e-02,  2.4305e-01,\n",
       "         -1.8980e-02, -2.7463e-01,  9.3368e-02,  2.6416e-01, -1.8338e-01,\n",
       "         -5.4863e-01,  4.3850e-01,  4.8306e-01, -3.9573e-01,  3.1816e-01,\n",
       "         -3.9107e-02,  3.9069e-02,  3.0239e-01,  5.0766e-01, -1.6244e-01,\n",
       "         -2.5461e-01, -2.6117e-01,  9.9081e-02,  1.6583e-01, -2.6066e-01,\n",
       "          4.7536e-01,  1.7053e-01,  2.2550e-01,  2.7020e-01, -1.6293e-01,\n",
       "          8.7827e-02,  5.2871e-04, -2.2224e-02, -2.7607e-01, -6.9855e-01,\n",
       "          3.1917e-01,  9.5289e-02,  9.9340e-02, -1.3135e-02, -3.6435e-01,\n",
       "          1.0387e-01, -1.2730e-01,  1.6310e-01, -5.5486e-01, -2.5397e-01,\n",
       "          8.9659e-02, -2.6428e-01,  3.1247e-01, -1.9158e-02,  3.5739e-01,\n",
       "         -1.8312e-01, -2.8196e-01,  2.6694e-01, -4.8782e-01, -6.7613e-02,\n",
       "         -2.4395e-01,  1.6271e-01, -2.1677e-01,  1.2505e-01,  5.4520e-01,\n",
       "         -9.5510e-02, -1.6452e-01, -3.5765e-01, -1.9290e-01, -1.2973e-01,\n",
       "         -3.4699e-01, -1.2847e-01,  5.7662e-02, -3.3334e-01,  1.5283e-01,\n",
       "          4.7766e-01, -6.2532e-01,  4.2679e-02, -9.2705e-03, -1.2374e-02,\n",
       "         -1.0937e-01,  5.6883e-01, -1.8587e-01,  6.1180e-01,  2.6160e-01,\n",
       "          1.6769e-01,  1.9866e-01, -3.4909e-01,  3.3676e-01, -4.7915e-02,\n",
       "         -3.0516e-01,  1.2031e-01,  3.4159e-02, -3.1160e-02, -1.4311e-01,\n",
       "         -5.8621e-01, -2.9877e-01,  1.7666e-01, -2.0402e-01, -9.7371e-03,\n",
       "         -3.0771e-03, -6.3804e-02, -2.0182e-01,  1.7288e-01,  1.6390e-02,\n",
       "         -8.4807e-02, -4.5207e-01, -4.3405e-01,  7.9389e-02, -2.1774e-01,\n",
       "          4.1532e-01, -2.2002e-01, -3.2166e-01,  5.0633e-01, -2.5192e-02,\n",
       "         -6.0008e-01, -1.8773e-02, -1.4752e-01,  4.9665e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = nn.Linear(64, 324)\n",
    "z(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a51ecf-417f-43d6-b9ae-6227ded7d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a: torch.Size([1, 6])\n",
      "shape of b: torch.Size([1, 6, 50])\n",
      "shape of c: torch.Size([1, 6, 64])\n",
      "shape of d: torch.Size([1, 1, 64])\n",
      "shape of e: torch.Size([1, 324])\n"
     ]
    }
   ],
   "source": [
    "x = nn.Embedding(324, embedding_dim=50)\n",
    "y = nn.RNN(50, 64, batch_first=True)\n",
    "z = nn.Linear(64, 324)\n",
    "\n",
    "a = dataset[0][0].reshape(1,6)\n",
    "print(\"shape of a:\", a.shape)\n",
    "b = x(a)\n",
    "print(\"shape of b:\", b.shape)\n",
    "c, d = y(b)\n",
    "print(\"shape of c:\", c.shape)\n",
    "print(\"shape of d:\", d.shape)\n",
    "\n",
    "e = z(d.squeeze(0))\n",
    "\n",
    "print(\"shape of e:\", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c4a140-418f-4f41-be27-5b82c64bd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
    "    self.rnn = nn.RNN(50, 64, batch_first=True)\n",
    "    self.fc = nn.Linear(64, vocab_size)\n",
    "\n",
    "  def forward(self, question):\n",
    "    embedded_question = self.embedding(question)\n",
    "    hidden, final = self.rnn(embedded_question)\n",
    "    output = self.fc(final.squeeze(0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3985234-667b-4aa0-bba5-76a517b39abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28841cea-c666-4739-8512-4e8f9c1fdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e94436-2f71-4f48-905d-e446f835db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set loss() and optimizer() \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f85f043-d008-44dc-b732-a880f49ebcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 528.600385\n",
      "Epoch: 2, Loss: 464.811750\n",
      "Epoch: 3, Loss: 391.569572\n",
      "Epoch: 4, Loss: 325.293901\n",
      "Epoch: 5, Loss: 269.970587\n",
      "Epoch: 6, Loss: 220.492341\n",
      "Epoch: 7, Loss: 174.902343\n",
      "Epoch: 8, Loss: 134.985900\n",
      "Epoch: 9, Loss: 103.030755\n",
      "Epoch: 10, Loss: 78.455251\n",
      "Epoch: 11, Loss: 60.026193\n",
      "Epoch: 12, Loss: 46.929373\n",
      "Epoch: 13, Loss: 37.217110\n",
      "Epoch: 14, Loss: 30.049087\n",
      "Epoch: 15, Loss: 24.479623\n",
      "Epoch: 16, Loss: 20.319222\n",
      "Epoch: 17, Loss: 17.141065\n",
      "Epoch: 18, Loss: 14.555485\n",
      "Epoch: 19, Loss: 12.543738\n",
      "Epoch: 20, Loss: 10.850525\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    "  for question, answer in dataloader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output = model(question)\n",
    "\n",
    "    # loss -> output shape (1,324) - (1)\n",
    "    loss = criterion(output, answer[0])\n",
    "\n",
    "    # gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch+1}, Loss: {total_loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df570a4b-8379-48a9-a2f2-0ab07b04024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold=0.02):\n",
    "\n",
    "  # convert question to numbers\n",
    "  numerical_question = text_to_indices(question, vocab)\n",
    "\n",
    "  # tensor\n",
    "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  output = model(question_tensor)\n",
    "\n",
    "  # convert logits to probs\n",
    "  probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "  # find index of max prob\n",
    "  value, index = torch.max(probs, dim=1)\n",
    "\n",
    "  if value < threshold:\n",
    "    print(\"I don't know\")\n",
    "\n",
    "  print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5dc5381-7666-48e3-8e36-5b116659087c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.keys())[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a60dc4d4-7e6a-4218-b76f-df0b4c84be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupiter\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"What is the largest planet in our solar system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbd534c7-f4df-4e11-85b6-cd5f2b26da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupiter\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"What planet our solar system?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d57733-e8c1-42b0-bef2-9562a0d320f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c426592-ffa8-4dee-950b-8df41c18e33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
